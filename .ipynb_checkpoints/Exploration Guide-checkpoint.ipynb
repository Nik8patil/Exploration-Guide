{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration guidelines for personal use.\n",
    "Information and code is copied from sources such as, Pedro Marcelino etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steps\n",
    "1. Understand the probelm\n",
    "2. Univariate analysis\n",
    "3. Multivariate analysis\n",
    "4. Basic cleaning\n",
    "5. Check assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the problem\n",
    "\n",
    "1. First check columns using // df.columns //\n",
    "2. Then make Excel Spreadsheet with following columns\n",
    "  <br> 1) Variable : Variable Name\n",
    "  <br> 2) Type : Numerical/Categorical\n",
    "  <br> 3) Segment : Make 3-4 or more variable segments. Eg for housing data three segments can be building, space and location.\n",
    "  <br> 4) Perceived Complexity: How complex dealing with/ Understanding variable will be.\n",
    "  <br> 5) Expectation/Usefulness : Expectation about variable influence in target variable. Can use high, medium and  low. (Important)   (Is the information already described in any other variable? If yes then Usefulness will be low.)\n",
    "  <br> 6) Conclusion : After checking data quickly any apparant changes in 5) (Do some scatter plots with high usefulness variables and check for correct conclusions.)\n",
    "  <br> 7) Comments : General Comments\n",
    "    \n",
    "This is not final outlook somecolumns with low may be more usefull. This is just to know data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Univariate Analysis\n",
    "\n",
    "1. First variable to analyse is always target variable.\n",
    "    <br> a) // df['target'].describe() // (Check if min max values are okay)\n",
    "    <br> b) // sns.distplot(df['target']) // (Check for normal distribution, Skewness, Peakedness)\n",
    "    <br> c) // print('Skewness: %f' % df['target'].skew() // (Check for +ve{tail on right} or -ve{tail on left} skew)\n",
    "    <br>    // print('Kurtosis: %f' % df['target'].kurt() // (Peakdeness and long,fat tails{3 is normal dist, >3 are tall)\n",
    "   \n",
    "2.  Relationship with other variables.\n",
    "   <br> a) Numerical\n",
    "        // var = 'varibale_name' \n",
    "          data = pd.concat([df['target'],df[var]],axis=1)\n",
    "          data.plot.scatter(x=var, y='target',ylim=(0, -) // (Check for high & mid if not many, This is to check relationship)\n",
    "    <br> b) Categorical\n",
    "         // var = 'varibale_name'\n",
    "          data = pd.concat([df['target'],df[var]],axis=1)\n",
    "          f, ax = plt.subplots(figsize = (8,6))\n",
    "          fig = sns.boxplot(x=var , y= 'target' , data=data)\n",
    "          fig.axis(ymin=0, ymax=-) // (Side by side boxplots to check relationship with target variable)\n",
    "          \n",
    "3. Write conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multivariate Analysis\n",
    "\n",
    "\n",
    "1) Heatmap: \n",
    "   <br>        // corrmat = df.corr()\n",
    "   <br>           f,ax = plt.subplots(figsize=(12,9))\n",
    "   <br>           sns.heatmap(corrmat,vmax=8,square=True) // ( Check correlation, Multicolinearity)\n",
    "    \n",
    "2) Heatmap of target, zoomed in, of most correlated variables:\n",
    "   <br>        // k=10 #number of variable in heatmap\n",
    "   <br>           cols = corrmat.nlargest(k, 'target')['taget'].index\n",
    "   <br>           cm = np.corrcoef(df[cols].values.T)   \n",
    "   <br>           sns.set(font_scale=1.25)\n",
    "   <br>           heatm = sns.heatmap(cm,cbar=True,annot=True,fmt='.2f',annot_kws={'size':10},yticklabels= cols.values, xticklabels=cols.values)\n",
    "   <br>           plt.show() // ( Check correlation, Multicolinearity)\n",
    "  \n",
    "3) Scatterplot:\n",
    "    <br>   // sns.set()\n",
    "    <br>      cols = ['name1','name2',etc]\n",
    "    <br>      sns.pairplot(df[cols],size=2.5)\n",
    "    <br>      plt.show()// (Multiple scatter plots)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Basic Cleaning\n",
    "\n",
    "1) Missing Data:\n",
    "\n",
    "a) Does missing data have pattern or is it random?<br>\n",
    "b) Total and Percent missing data:\n",
    "# Create table for missing data analysis\n",
    "def draw_missing_data_table(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    return missing_data\n",
    "    <br> // total = df.isnull().sum().sort_values(ascending=False)\n",
    "    <br>    percent = (df.isnuall().sum()/df.isnull().count().sort_values(ascending=False)\n",
    "    <br>    missing_data = pd.concat([total,percent], axis=1,keys=['Total', 'Percent'])\n",
    "    <br>    missing_data.head()//  ( Check and delete columns and rows accordingly)\n",
    "  \n",
    "2) Outliers:  \n",
    "\n",
    "a) Univariate:\n",
    "    <br> Code below is code that standardizes data, such that values have mean 0 and std of 1. \n",
    "    <br> // scaled = StandardScaler().fit_transform(df['target'][:,np.newaxis])\n",
    "    <br>    low_range = scaled[scaled][:,0].argsort()][:10]\n",
    "    <br>    high_range = scaled[scaled][:,0].argsort()][:-10]\n",
    "    <br>    print('Outer range (low) of the distribution')\n",
    "    <br>    print(low_range)\n",
    "    <br>    print('Outer range (high) of the distribution')\n",
    "    <br>    print(high_range)//\n",
    "    <br>    (Values way to high or low than standard -3 to 3 are outliers)\n",
    "<br>\n",
    "b) Bivariate:\n",
    "    <br> Scatterplot\n",
    "    <br> // vart= 'column_name'\n",
    "    <br>    data =  pd.concat(df['target'],df[vart]],axis=1)\n",
    "    <br>    data.plot.scatter(x=vart, y='target', ylim=(0,num)//\n",
    "    <br>    (See  if data points are following thrend or not. And remove them accordingly)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Check Assumptions\n",
    "\n",
    "1) Normality: Check univariate normality for 'target' (which is a limited approach). Univariate normality doesn't ensure   multivariate normality (which is what we would like to have), but it helps. In big samples (>200 observations) normality is not such an issue.\n",
    "<br>2) Homoscadasticity: Assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable\n",
    "<br>3) Linearity: If patterns are not linear, it would be worthwhile to explore data transformations.\n",
    "<br>4) Absence of correlated errors: Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n",
    "\n",
    "1) Noramality:\n",
    "    Check noramality by a) Histogram b) Normal probability plot:\n",
    "    <br> //sns.distplot(df['target'],fit=norm)\n",
    "    <br>   fig = plt.figure()\n",
    "    <br>   res = stats.probplot(df['target'],plot=plt) //\n",
    "    <br> If target is not linear & we need it to be we can apply simple data transformation. For positivev skew, log transformation usually works well.\n",
    "    \n",
    "   <br> // df['target'] = np.log(df['target']) //\n",
    "   <br>    Then check again with same code as above.\n",
    "    \n",
    "   <br> Then check for other variables and do the same. \n",
    "   <br> In case where lots of observations are zero, log transformation doesn't work with zero values.\n",
    "   (To apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.)\n",
    "   \n",
    "   <br> // #create column for new variable (one is enough because it's a binary categorical feature)\n",
    "   <br>    #if area>0 it gets 1, for area==0 it gets 0\n",
    "   <br> df['new_var_name'] = pd.Series(len(df_train['var_name']), index=df.index)\n",
    "   <br> df['new_var_name'] = 0 \n",
    "   <br> df.loc[df_train['var_name']>0,'new_var_name'] = 1 //\n",
    "   \n",
    "   <br> Then apply transformation:\n",
    "   <br> // df.loc[df['new_var_name']==1, 'var_name'] = np.log(df['var_name']) // \n",
    "   <br> Then check again with same code as first.\n",
    "   \n",
    "<br>\n",
    "2) Homoscedasticity: \n",
    "   <br> The best approach to test homoscedasticity for two metric variables is graphically(Scatter plot). Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n",
    "   <br> Just by ensuring normality in some variables, we could solve the homoscedasticity problem. But not for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummies\n",
    "\n",
    "// df = pd.get_dummies(df) // "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
